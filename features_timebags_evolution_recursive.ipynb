{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# How does the distribution of features evolve when we go through the timebags?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Some loading and prep code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First some imports\n",
      "import numpy as np\n",
      "import matplotlib as mpl\n",
      "import seaborn as sb\n",
      "%matplotlib inline\n",
      "\n",
      "import settings as st\n",
      "from linguistics.treetagger import get_tagger\n",
      "from linguistics.wn import lemmatize as relemmatize\n",
      "from datainterface.redistools import RedisReader\n",
      "import datainterface.picklesaver as ps\n",
      "from util.generic import ProgressInfo\n",
      "tagger = get_tagger()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load the clusters, the features, and initialize the results\n",
      "clusters = RedisReader(st.redis_mt_clusters_filtered_pref)\n",
      "features = dict((fsrc, dict((ftype, ps.load(fdetails['file'].format('all')))\n",
      "                            for ftype, fdetails in fsrc_details.iteritems()))\n",
      "                for fsrc, fsrc_details in st.mt_analysis_features.iteritems())\n",
      "empirical_features = dict((fsrc, dict((ftype, [])\n",
      "                                      for ftype in fsrc_values.iterkeys()))\n",
      "                          for fsrc, fsrc_values in features.iteritems())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from linguistics.distance import hamming_word\n",
      "    \n",
      "def is_in_sphere(ref_strings, string):\n",
      "    \"\"\"Whether or not `string` is in the hamming_word-sphere at\n",
      "    distance 1 surrounding the `ref_strings`.\"\"\"\n",
      "    \n",
      "    for ref_str in ref_strings:\n",
      "        if hamming_word(ref_str, string) == 1:\n",
      "            return True\n",
      "    return False\n",
      "\n",
      "\n",
      "def get_filtered_relems_and_strings(tbg, ref_strings, excl_strings):\n",
      "    \"\"\"List the relemmatized words (once per occurrence) appearing in `tbg`,\n",
      "    and at distance one from `ref_strings`, excluding strings in `excl_strings`.\n",
      "    \n",
      "    Quotes are first lemmatized with the TreeTagger lemmatizer, then\n",
      "    relemmatized as in the mining analysis, with the WordNet morphy\n",
      "    method.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    tbg : TimeBag\n",
      "        The timebag to work on.\n",
      "    ref_strings : list of strings\n",
      "        The reference string from which to compute distance; if None, then ignored.\n",
      "    excl_strings : list of strings\n",
      "        The reference string from which to exclude; if None, then ignored.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    list\n",
      "        The list of relemmatized words.\n",
      "    set\n",
      "        The set of new strings considered to the product of this iteration.\n",
      "    \n",
      "    \"\"\"\n",
      "    \n",
      "    filtered_relems = []\n",
      "    new_strings = set([])\n",
      "\n",
      "    for string, freq in zip(tbg.strings, tbg.tot_freqs):\n",
      "        # If our current string is in the set of banned strings,\n",
      "        # jump to next one\n",
      "        if excl_strings is not None and string in excl_strings:\n",
      "            continue\n",
      "        \n",
      "        # If our current string is not in the sphere around ref_strings,\n",
      "        # jump to next one\n",
      "        if ref_strings is not None and not is_in_sphere(ref_strings, string):\n",
      "            continue\n",
      "\n",
      "        new_strings.add(string)\n",
      "        relems = [relemmatize(lem) for lem in tagger.Lemmatize(string)]\n",
      "        filtered_relems.extend(relems * freq)\n",
      "\n",
      "    return filtered_relems, new_strings"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The analysis itself\n",
      "\n",
      "We want to look at the distribution of features in *new* quotes produced by susbtitution (i.e. not those reinjected from outside as part of the continuous process). So for each cluster, we look at the first timebag, and take it as a reference and include all quotes in the second timebag that aren't in the first but are at distance *d* from a quote in the first. We repeat this operation for each later timebag, but basing ourselves on the set of new quotes produced by the previous step. Then on each of these sets of quotes, we compute the distribution of features. This gives us the distribution of features on the novel quotes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Duration of a timebag, in days\n",
      "timebag_size = 4.0\n",
      "# Minimum number of timebags a cluster must have to be included in the analysis\n",
      "min_number_of_timebags = 5\n",
      "# Number of timebags included in 'new quotes' plot below\n",
      "n_tbgs_new_quotes = 5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    #raise IOError\n",
      "    # See if we already computed this\n",
      "    empirical_features = ps.load('data/analyses/empirical_features_recursive_min-5-tbgs_tbgsize-4_no-exclusion.pickle')\n",
      "    np_new_quotes_counts = ps.load('data/analyses/new_quotes_counts_recursive_min-5-tbgs_tbgsize-4_no-exclusion.pickle')\n",
      "    print 'Loaded from pickle.'\n",
      "except IOError:\n",
      "    n_clusters_kept = 0\n",
      "    new_quotes_counts = []\n",
      "    print_clusters = []\n",
      "    \n",
      "    # A progress indicator\n",
      "    progress = ProgressInfo(len(clusters), 10, 'clusters')\n",
      "    \n",
      "    # For each cluster\n",
      "    for c in clusters.itervalues():\n",
      "        progress.next_step()\n",
      "        \n",
      "        # Build timebags, skip if we don't have enough bags\n",
      "        tbgs = c.build_timebags(timebag_size)\n",
      "        if len(tbgs) < min_number_of_timebags:\n",
      "            continue\n",
      "        \n",
      "        print_cluster = (c, [])\n",
      "        max_new_qts_per_tbg = 0\n",
      "        \n",
      "        n_clusters_kept += 1\n",
      "        tbgs_n_new_quotes = []\n",
      "        for i, tbg in enumerate(tbgs):\n",
      "            # Update the set of base strings considered as we go\n",
      "            filtered_relems, new_strings = get_filtered_relems_and_strings(tbg,\n",
      "                                                                           None if i == 0 else new_strings,\n",
      "                                                                           None) # no exclusion\n",
      "            prevtbg = tbg\n",
      "\n",
      "            if i > 0:\n",
      "                max_new_qts_per_tbg = max(max_new_qts_per_tbg, len(new_strings))\n",
      "            print_cluster[1].append(([q for q in c.quotes.itervalues() if q.string in tbg.strings],\n",
      "                                     [q for q in c.quotes.itervalues() if q.string in new_strings]))\n",
      "            \n",
      "            # Update number of new strings\n",
      "            if 0 < i and i < n_tbgs_new_quotes:\n",
      "                tbgs_n_new_quotes.append(len(new_strings))\n",
      "            \n",
      "            # For each feature\n",
      "            for fsrc, fsrc_values in features.iteritems():\n",
      "                for ftype, feature in fsrc_values.iteritems():\n",
      "            \n",
      "                    # Get the feature values for the extracted words\n",
      "                    tbg_empirical_features = [feature[relem] for relem in filtered_relems if relem in feature]\n",
      "                    try:\n",
      "                        # Either add it to the existing list\n",
      "                        empirical_features[fsrc][ftype][i].extend(tbg_empirical_features)\n",
      "                    except IndexError:\n",
      "                        # Or create the list for this timebag if necessary\n",
      "                        empirical_features[fsrc][ftype].append(tbg_empirical_features)\n",
      "                        # Then check that the item we just created was indeed for the right timebag.\n",
      "                        empirical_features[fsrc][ftype][i]\n",
      "                        \n",
      "        new_quotes_counts.append(np.array(tbgs_n_new_quotes))\n",
      "        if max_new_qts_per_tbg > 0:\n",
      "            print_clusters.append(print_cluster)\n",
      "    \n",
      "    # Convert to numpy arrays\n",
      "    print 'Kept {} clusters.'.format(n_clusters_kept)\n",
      "    np_new_quotes_counts = np.array(new_quotes_counts)\n",
      "    for fsrc, fsrc_values in features.iteritems():\n",
      "        for ftype in fsrc_values.iterkeys():\n",
      "            for i, all_tbg_empirical_features in enumerate(empirical_features[fsrc][ftype]):\n",
      "                empirical_features[fsrc][ftype][i] = np.array(all_tbg_empirical_features)\n",
      "    \n",
      "    # Save to disk\n",
      "    ps.save(empirical_features, 'data/analyses/empirical_features_recursive_min-5-tbgs_tbgsize-4_no-exclusion.pickle')\n",
      "    ps.save(np_new_quotes_counts, 'data/analyses/new_quotes_counts_recursive_min-5-tbgs_tbgsize-4_no-exclusion.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "bins = np.linspace(0, np_new_quotes_counts.max() + 1, np_new_quotes_counts.max() + 2) - 0.5\n",
      "fig, axs = plt.subplots(1, min_number_of_timebags - 1, figsize=(2 * (min_number_of_timebags - 1), 2), sharey=True)\n",
      "for i in range(min_number_of_timebags - 1):\n",
      "    axs[i].hist(np_new_quotes_counts[:, i], bins=bins, log=True)\n",
      "    axs[i].set_xlim(bins[0], bins[-1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(print_clusters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Find until which timebag clusters keep having new quotes\n",
      "last_tbg_new_quotes = np.zeros(len(print_clusters), dtype=int)\n",
      "for i, (c, tbgs_details) in enumerate(print_clusters):\n",
      "    last_tbg_new_quotes[i] = np.array([len(tbg_details[1]) for tbg_details in tbgs_details]).nonzero()[0][-1]\n",
      "\n",
      "bins = np.linspace(- 0.5, 20 + 0.5, 20 + 2)\n",
      "plt.hist(last_tbg_new_quotes, bins=bins, log=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_cluster_novelty(c, tbgs_details):\n",
      "    n_tbgs_shown = min_number_of_timebags\n",
      "    blue = sb.color_palette()[5]\n",
      "    red = sb.color_palette()[2]\n",
      "    \n",
      "    fig, ax = plt.subplots(1, 1, figsize=(15, 3))\n",
      "    ax.set_title('Cluster #{}, {} quotes, total freq {}'.format(c.id, c.n_quotes, c.tot_freq))\n",
      "    ax.set_xlim(- 0.5, n_tbgs_shown - 0.5)\n",
      "    ax.set_ylim(- 0.5, c.n_quotes - 0.5)\n",
      "    ax.set_xticks(range(n_tbgs_shown))\n",
      "    ax.set_yticks(range(c.n_quotes))\n",
      "    \n",
      "    qt_coords = dict((q.id, i) for i, q in enumerate(c.quotes.itervalues()))\n",
      "    for i, tbg_details in enumerate(tbgs_details[:n_tbgs_shown]):\n",
      "        tbg_qt_coords = np.array([qt_coords[q.id] for q in tbg_details[0]])\n",
      "        tbg_new_qt_coords = np.array([qt_coords[q.id] for q in tbg_details[1]])\n",
      "        ax.plot(np.ones_like(tbg_qt_coords) * i, tbg_qt_coords, 'o', color=blue, markersize=8)\n",
      "        ax.plot(np.ones_like(tbg_new_qt_coords) * i, tbg_new_qt_coords, 'o', color=red, markersize=12)\n",
      "    fig.show()\n",
      "\n",
      "\n",
      "#plot_cluster_novelty(print_clusters[1][0], print_clusters[1][1])\n",
      "for i in np.where(last_tbg_new_quotes >= min_number_of_timebags - 1)[0]:\n",
      "    c, tbgs_details = print_clusters[i]\n",
      "    plot_cluster_novelty(c, tbgs_details)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in np.where(last_tbg_new_quotes >= min_number_of_timebags - 2)[0]:\n",
      "    c, tbgs_details = print_clusters[i]\n",
      "    print '### Cluster #{}, {} quotes, total freq {}'.format(c.id, c.n_quotes, c.tot_freq)\n",
      "    for i, tbg_details in enumerate(tbgs_details):\n",
      "        if i >= 10:\n",
      "            break\n",
      "        print ' ## Bag {}'.format(i)\n",
      "        for q in tbg_details[0]:\n",
      "            print '  {} {} [#{}]'.format('n' if q in tbg_details[1] else ' ', q.string, q.id)\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## A quick check\n",
      "\n",
      "Just to have a look, here we count the average number of *new* quotes that appear in the first ten timebags of each cluster (*new* according to the definition above)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.arange(1, n_tbgs_new_quotes)\n",
      "y = np_new_quotes_counts.mean(0)\n",
      "ci = 1.96 * np_new_quotes_counts.std(0) / np.sqrt(len(np_new_quotes_counts[:, 0]) - 1)\n",
      "mpl.pyplot.plot(x, np_new_quotes_counts.mean(0))\n",
      "mpl.pyplot.fill_between(x, y - ci, y + ci, alpha=0.2, color='grey')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}