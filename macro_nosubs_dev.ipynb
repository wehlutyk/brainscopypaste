{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# How does the distribution of features evolve in time?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Load and prep"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Duration of a timebag, in days\n",
      "timebag_size = 1.0\n",
      "# Minimum number of days a cluster must have to be included in the analysis\n",
      "min_days = 0\n",
      "# Maximum number of days a cluster must have to be included in the analysis\n",
      "max_days = 80\n",
      "# Clusters filtering type to use\n",
      "filtering_type = 'filtered'\n",
      "\n",
      "file_postfix = ('_nosubs_F{filtering_type}_R{min_days}-{max_days}_'\n",
      "                'D{timebag_size}').format(filtering_type=filtering_type,\n",
      "                                           min_days=min_days, max_days=max_days,\n",
      "                                           timebag_size=timebag_size)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First some imports\n",
      "import textwrap\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib as mpl\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sb\n",
      "%matplotlib inline\n",
      "\n",
      "import settings as st\n",
      "from linguistics.treetagger import get_tagger\n",
      "from linguistics.wn import lemmatize as relemmatize\n",
      "from linguistics.distance import levenshtein\n",
      "from datainterface.redistools import RedisReader\n",
      "import datainterface.picklesaver as ps\n",
      "from util.generic import ProgressInfo, is_int\n",
      "tagger = get_tagger()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load the clusters, the features, and initialize the results\n",
      "clusters = RedisReader(st.__getattribute__('redis_mt_clusters_{}_pref'.format(filtering_type)))\n",
      "features = dict((fsrc, dict((ftype, ps.load(fdetails['file'].format('all')))\n",
      "                            for ftype, fdetails in fsrc_details.iteritems()))\n",
      "                for fsrc, fsrc_details in st.mt_analysis_features.iteritems())\n",
      "macro_features = dict((fsrc, dict((ftype, [])\n",
      "                                  for ftype in fsrc_values.iterkeys()))\n",
      "                      for fsrc, fsrc_values in features.iteritems())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_relems(tbg):\n",
      "    \"\"\"List the relemmatized words (once per quote) appearing in `tbg`.\"\"\"\n",
      "    #\"\"\"List the relemmatized words (once per occurrence) appearing in `tbg`.\"\"\"\n",
      "    \n",
      "    tbg_relems = []\n",
      "    \n",
      "    for string, freq in zip(tbg.strings, tbg.tot_freqs):\n",
      "        string_relems = [relemmatize(lem) for lem in tagger.Lemmatize(string)]\n",
      "        tbg_relems.extend(string_relems)# * freq)\n",
      "\n",
      "    return tbg_relems"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Mining"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    raise IOError\n",
      "    # See if we already computed this\n",
      "    macro_features = ps.load('data/MemeTracker/macro/macro_features{}.pickle'.format(file_postfix))\n",
      "    print 'Loaded from pickle.'\n",
      "except IOError:\n",
      "    n_clusters_kept = 0\n",
      "    progress = ProgressInfo(len(clusters), 10, 'clusters')\n",
      "    \n",
      "    for c in clusters.itervalues():\n",
      "        progress.next_step()\n",
      "        \n",
      "        # Skip if out of min-max\n",
      "        c.build_timeline()\n",
      "        if c.timeline.span_days > max_days or c.timeline.span_days < min_days:\n",
      "            continue\n",
      "        n_clusters_kept += 1\n",
      "        \n",
      "        tbgs = c.build_timebags(timebag_size)\n",
      "        for i, tbg in enumerate(tbgs):\n",
      "            # Relems\n",
      "            relems = get_relems(tbg)\n",
      "            \n",
      "            # Features\n",
      "            for fsrc, fsrc_values in features.iteritems():\n",
      "                for ftype, feature in fsrc_values.iteritems():\n",
      "            \n",
      "                    # Get the feature values for the extracted words\n",
      "                    macro_features_tbg_this_cluster = [feature[relem] for relem in relems if relem in feature]\n",
      "                    try:\n",
      "                        # Either add it to the existing list\n",
      "                        macro_features[fsrc][ftype][i].extend(macro_features_tbg_this_cluster)\n",
      "                    except IndexError:\n",
      "                        # Or create the list for this timebag if necessary\n",
      "                        macro_features[fsrc][ftype].append(macro_features_tbg_this_cluster)\n",
      "                        # Then check that the item we just created was indeed for the right timebag.\n",
      "                        macro_features[fsrc][ftype][i]\n",
      "    \n",
      "    # Convert to numpy arrays\n",
      "    print 'Kept {} clusters from days filter ({}-{}).'.format(n_clusters_kept, min_days, max_days)\n",
      "    for fsrc, fsrc_values in features.iteritems():\n",
      "        for ftype in fsrc_values.iterkeys():\n",
      "            for i, macro_features_tbg in enumerate(macro_features[fsrc][ftype]):\n",
      "                macro_features[fsrc][ftype][i] = np.array(macro_features_tbg)\n",
      "    \n",
      "    # Save to disk\n",
      "    ps.save(macro_features, 'data/MemeTracker/macro/macro_features{}.pickle'.format(file_postfix))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  10 % (1962 / 19621 clusters)\n",
        "  20 % (3924 / 19621 clusters)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  30 % (5886 / 19621 clusters)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  40 % (7848 / 19621 clusters)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  50 % (9810 / 19621 clusters)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  60 % (11772 / 19621 clusters)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  70 % (13734 / 19621 clusters)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  80 % (15696 / 19621 clusters)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  90 % (17658 / 19621 clusters)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  100 % (19620 / 19621 clusters)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Kept 19621 clusters from days filter (0-80).\n"
       ]
      }
     ],
     "prompt_number": 5
    }
   ],
   "metadata": {}
  }
 ]
}