{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# How does the distribution of features evolve when we go through the timebags?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Some loading and prep code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First some imports\n",
      "import numpy as np\n",
      "import matplotlib as mpl\n",
      "import seaborn as sb\n",
      "%matplotlib inline\n",
      "\n",
      "import settings as st\n",
      "from linguistics.treetagger import get_tagger\n",
      "from linguistics.wn import lemmatize as relemmatize\n",
      "from datainterface.redistools import RedisReader\n",
      "from datainterface.fs import check_file, get_filename\n",
      "import datainterface.picklesaver as ps\n",
      "from datastructure.full import QtString\n",
      "from util.generic import ProgressInfo\n",
      "from baseargs import BaseArgs\n",
      "tagger = get_tagger()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load the clusters, substitutions, the features, and initialize the results\n",
      "bargs = BaseArgs({'ff': 'filtered',\n",
      "                  'POS': 'all',\n",
      "                  'model': 'tbgs',\n",
      "                  'substrings': False,\n",
      "                  'timebag_size': 2.0})\n",
      "substitutions_filename = get_filename(bargs)\n",
      "check_file(substitutions_filename, for_read=True)\n",
      "substitutions = ps.load(substitutions_filename)\n",
      "clusters = RedisReader(st.redis_mt_clusters_filtered_pref)\n",
      "features = dict((fsrc, dict((ftype, ps.load(fdetails['file'].format('all')))\n",
      "                            for ftype, fdetails in fsrc_details.iteritems()))\n",
      "                for fsrc, fsrc_details in st.mt_analysis_features.iteritems())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Convert to palatable structure\n",
      "arrival_quotes = np.array([s.daughter for s in substitutions], dtype=QtString)\n",
      "arrival_tbg_idxs = np.array([s.mining_info['bag2'] for s in substitutions])\n",
      "s_order = np.argsort(arrival_tbg_idxs)\n",
      "arrival_tbg_idxs = arrival_tbg_idxs[s_order]\n",
      "arrival_quotes = arrival_quotes[s_order]\n",
      "highest_tbg_idx = arrival_tbg_idxs[-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Build the diffusion graph to filter out only the quotes we want\n",
      "import networkx as nx\n",
      "edges = {}\n",
      "for s in substitutions:\n",
      "    id_mother = str(int(s.mother.qt_id)) + ':' + str(s.mining_info['bag1'])\n",
      "    id_daughter = str(int(s.daughter.qt_id)) + ':' + str(s.mining_info['bag2'])\n",
      "    try:\n",
      "        edges[id_mother].add(id_daughter)\n",
      "    except KeyError:\n",
      "        edges[id_mother] = set([id_daughter])\n",
      "diffusion_graph = nx.from_dict_of_lists(edges, create_using=nx.DiGraph())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_filtered_relems(strings, freqs=None):\n",
      "    \"\"\"List the relemmatized words (once per occurrence) appearing in `strings`.\n",
      "    \n",
      "    Strings are first lemmatized with the TreeTagger lemmatizer, then\n",
      "    relemmatized as in the mining analysis, with the WordNet morphy\n",
      "    method.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    strings : string or list of strings\n",
      "        The string(s) to work on.\n",
      "    freqs : list of ints, optional\n",
      "        List of frequencies to ponder the strings with; defaults to a list of ones.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    list\n",
      "        The list of relemmatized words.\n",
      "    \n",
      "    \"\"\"\n",
      "    \n",
      "    relems = []\n",
      "    if not isinstance(strings, list):\n",
      "        strings = [strings]\n",
      "    if freqs is None:\n",
      "        freqs = np.ones(len(strings))\n",
      "\n",
      "    for string, freq in zip(strings, freqs):\n",
      "        relems = [relemmatize(lem) for lem in tagger.Lemmatize(string)]\n",
      "        relems.extend(relems * freq)\n",
      "\n",
      "    return relems"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def is_direct_child(base_qtbg_ids, target_qtbg_id):\n",
      "    \"\"\"Is `target_qtbg_id` a direct child from any of `base_qtbg_ids` in the\n",
      "    diffusion graph or not.\"\"\"\n",
      "    \n",
      "    in_nodes = set([e[0] for e in diffusion_graph.in_edges(target_qtbg_id)])\n",
      "    return len(in_nodes.intersection(base_qtbg_ids)) > 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The analysis itself\n",
      "\n",
      "We want to look at the distribution of features in *new* quotes produced by susbtitution (i.e. not those reinjected from outside as part of the continuous process). So for each cluster, we look at the first timebag, and take it as a reference and include all quotes in the second timebag that aren't in the first but are at distance *d* from a quote in the first. We repeat this operation for each later timebag, but basing ourselves on the set of new quotes produced by the previous step. Then on each of these sets of quotes, we compute the distribution of features. This gives us the distribution of features on the novel quotes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Duration of a timebag, in days\n",
      "timebag_size = bargs.timebag_size\n",
      "# Number of timebags included in 'new quotes' plot below.\n",
      "n_tbgs_new_quotes = 20"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    # See if we already computed this\n",
      "    tbg0_all_empirical_features = ps.load('data/analyses/tbg0_all_empirical_features.pickle')\n",
      "    print 'Loaded from pickle.'\n",
      "except IOError:\n",
      "    progress_init = ProgressInfo(len(clusters), 10, 'clusters')\n",
      "    tbg0_all_empirical_features = {}\n",
      "    relems = []\n",
      "    for c in clusters.itervalues():\n",
      "        progress_init.next_step()\n",
      "        tbg0 = c.build_timebags(timebag_size)[0]\n",
      "        relems += get_filtered_relems(tbg0.strings, tbg0.tot_freqs)\n",
      "    for fsrc, fsrc_values in features.iteritems():\n",
      "        try:\n",
      "            tbg0_all_empirical_features[fsrc]\n",
      "        except KeyError:\n",
      "            tbg0_all_empirical_features[fsrc] = {}\n",
      "        for ftype, feature in fsrc_values.iteritems():\n",
      "            tbg0_all_empirical_features[fsrc][ftype] = np.array([feature[relem] for relem in relems if relem in feature])\n",
      "    \n",
      "    # Save to disk\n",
      "    ps.save(tbg0_all_empirical_features, 'data/analyses/tbg0_all_empirical_features.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loaded from pickle.\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    # See if we already computed this\n",
      "    empirical_features = ps.load('data/analyses/empirical_features_from_mining_recursive.pickle')\n",
      "    np_new_quotes_counts = ps.load('data/analyses/new_quotes_counts_from_mining_recursive.pickle')\n",
      "    print 'Loaded from pickle.'\n",
      "except IOError:\n",
      "    cl_new_quotes_counts = {}\n",
      "    empirical_features = {}\n",
      "    n_clusters_extracted = 0\n",
      "    tbg0_qtbg_ids = set([str(int(s.mother.qt_id)) + ':0'\n",
      "                         for s in substitutions if s.mining_info['bag1'] == 0])\n",
      "    \n",
      "    # Initialization for tbg0\n",
      "    print 'Init step'\n",
      "    for fsrc, fsrc_values in features.iteritems():\n",
      "        try:\n",
      "            empirical_features[fsrc]\n",
      "        except KeyError:\n",
      "            empirical_features[fsrc] = {}\n",
      "        for ftype, feature in fsrc_values.iteritems():\n",
      "            empirical_features[fsrc][ftype] = [tbg0_all_empirical_features[fsrc][ftype]]\n",
      "    \n",
      "    # Progress indicator\n",
      "    print 'The rest'\n",
      "    progress = ProgressInfo(len(arrival_tbg_idxs), 10, 'substitutions')\n",
      "    \n",
      "    # For each substitution\n",
      "    base_qtbg_ids = tbg0_qtbg_ids\n",
      "    current_target_tbg_idx = 1\n",
      "    print ('Starting with timebag {}, with {} base quotes '\n",
      "           'from timebag {}').format(current_target_tbg_idx,\n",
      "                                     len(base_qtbg_ids),\n",
      "                                     current_target_tbg_idx - 1)\n",
      "    for i, (tbg_idx, qt) in enumerate(zip(arrival_tbg_idxs, arrival_quotes)):\n",
      "        progress.next_step()\n",
      "        \n",
      "        while tbg_idx > current_target_tbg_idx:\n",
      "            base_qtbg_ids = set([e[1] for base_qtbg_id in base_qtbg_ids\n",
      "                                 for e in diffusion_graph.out_edges(base_qtbg_id)])\n",
      "            current_target_tbg_idx += 1\n",
      "            print ('Stepped to timebag {}, now with {} base quotes '\n",
      "                   'from timebag {}').format(current_target_tbg_idx,\n",
      "                                             len(base_qtbg_ids),\n",
      "                                             current_target_tbg_idx - 1)\n",
      "        \n",
      "        if not is_direct_child(base_qtbg_ids, str(int(qt.qt_id)) + ':{}'.format(current_target_tbg_idx)):\n",
      "            continue\n",
      "        \n",
      "        if 0 < tbg_idx:\n",
      "            try:\n",
      "                cl_new_quotes_counts[qt.cl_id][tbg_idx - 1] += 1\n",
      "            except KeyError:\n",
      "                # First time we meet this cluster\n",
      "                n_clusters_extracted += 1\n",
      "                cl_new_quotes_counts[qt.cl_id] = [0] * tbg_idx\n",
      "                cl_new_quotes_counts[qt.cl_id][tbg_idx - 1] += 1\n",
      "            except IndexError:\n",
      "                # Cluster already met, but timebag list not long enough. Add the necessary zeros.\n",
      "                l = len(cl_new_quotes_counts[qt.cl_id])\n",
      "                cl_new_quotes_counts[qt.cl_id].extend([0] * (tbg_idx - l))\n",
      "                cl_new_quotes_counts[qt.cl_id][tbg_idx - 1] += 1\n",
      "    \n",
      "        relems = get_filtered_relems(qt)\n",
      "        \n",
      "        # For each feature\n",
      "        for fsrc, fsrc_values in features.iteritems():\n",
      "            for ftype, feature in fsrc_values.iteritems():\n",
      "        \n",
      "                # Get the feature values for the extracted words\n",
      "                this_empirical_features = [feature[relem] for relem in relems if relem in feature]\n",
      "                try:\n",
      "                    # Either add it to the existing list\n",
      "                    empirical_features[fsrc][ftype][tbg_idx].extend(this_empirical_features)\n",
      "                except IndexError:\n",
      "                    # Or create the list for this timebag if necessary\n",
      "                    empirical_features[fsrc][ftype].append(this_empirical_features)\n",
      "                    # Then check that the item we just created was indeed for the right timebag.\n",
      "                    empirical_features[fsrc][ftype][tbg_idx]\n",
      "    \n",
      "    # Convert to numpy arrays\n",
      "    print 'Extracted from {} clusters.'.format(n_clusters_extracted)\n",
      "    \n",
      "    new_quotes_counts = []\n",
      "    for cl_id, cl_n_new_quotes in cl_new_quotes_counts.iteritems():\n",
      "        # If we had enough timebags, keep this for the 'new quotes' counts\n",
      "        if len(cl_n_new_quotes) >= n_tbgs_new_quotes - 1:\n",
      "            new_quotes_counts.append(np.array(cl_n_new_quotes[:n_tbgs_new_quotes - 1]))\n",
      "    np_new_quotes_counts = np.array(new_quotes_counts)\n",
      "    \n",
      "    for fsrc, fsrc_values in features.iteritems():\n",
      "        for ftype in fsrc_values.iterkeys():\n",
      "            for i, all_tbg_empirical_features in enumerate(empirical_features[fsrc][ftype]):\n",
      "                empirical_features[fsrc][ftype][i] = np.array(all_tbg_empirical_features)\n",
      "    \n",
      "    # Save to disk\n",
      "    ps.save(empirical_features, 'data/analyses/empirical_features_from_mining_recursive.pickle')\n",
      "    ps.save(np_new_quotes_counts, 'data/analyses/new_quotes_counts_from_mining_recursive.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Init step\n",
        "The rest\n",
        "Starting with timebag 1, with 441 base quotes from timebag 0\n",
        "Stepped to timebag 2, now with 462 base quotes from timebag 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Stepped to timebag 3, now with 67 base quotes from timebag 2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Stepped to timebag 4, now with 12 base quotes from timebag 3\n",
        "  10 % (1023 / 10225 substitutions)\n",
        "Stepped to timebag 5, now with 2 base quotes from timebag 4\n",
        "Stepped to timebag 6, now with 0 base quotes from timebag 5\n",
        "Stepped to timebag 7, now with 0 base quotes from timebag 6\n",
        "Stepped to timebag 8, now with 0 base quotes from timebag 7\n",
        "Stepped to timebag 9, now with 0 base quotes from timebag 8\n",
        "Stepped to timebag 10, now with 0 base quotes from timebag 9\n",
        "  20 % (2046 / 10225 substitutions)\n",
        "Stepped to timebag 11, now with 0 base quotes from timebag 10\n",
        "Stepped to timebag 12, now with 0 base quotes from timebag 11\n",
        "Stepped to timebag 13, now with 0 base quotes from timebag 12\n",
        "Stepped to timebag 14, now with 0 base quotes from timebag 13\n",
        "Stepped to timebag 15, now with 0 base quotes from timebag 14\n",
        "Stepped to timebag 16, now with 0 base quotes from timebag 15\n",
        "Stepped to timebag 17, now with 0 base quotes from timebag 16\n",
        "Stepped to timebag 18, now with 0 base quotes from timebag 17\n",
        "Stepped to timebag 19, now with 0 base quotes from timebag 18\n",
        "  30 % (3069 / 10225 substitutions)\n",
        "Stepped to timebag 20, now with 0 base quotes from timebag 19\n",
        "Stepped to timebag 21, now with 0 base quotes from timebag 20\n",
        "Stepped to timebag 22, now with 0 base quotes from timebag 21\n",
        "Stepped to timebag 23, now with 0 base quotes from timebag 22\n",
        "Stepped to timebag 24, now with 0 base quotes from timebag 23\n",
        "Stepped to timebag 25, now with 0 base quotes from timebag 24\n",
        "Stepped to timebag 26, now with 0 base quotes from timebag 25\n",
        "Stepped to timebag 27, now with 0 base quotes from timebag 26\n",
        "  40 % (4092 / 10225 substitutions)\n",
        "Stepped to timebag 28, now with 0 base quotes from timebag 27\n",
        "Stepped to timebag 29, now with 0 base quotes from timebag 28\n",
        "Stepped to timebag 30, now with 0 base quotes from timebag 29\n",
        "Stepped to timebag 31, now with 0 base quotes from timebag 30\n",
        "Stepped to timebag 32, now with 0 base quotes from timebag 31\n",
        "Stepped to timebag 33, now with 0 base quotes from timebag 32\n",
        "Stepped to timebag 34, now with 0 base quotes from timebag 33\n",
        "Stepped to timebag 35, now with 0 base quotes from timebag 34\n",
        "Stepped to timebag 36, now with 0 base quotes from timebag 35\n",
        "  50 % (5115 / 10225 substitutions)\n",
        "Stepped to timebag 37, now with 0 base quotes from timebag 36\n",
        "Stepped to timebag 38, now with 0 base quotes from timebag 37\n",
        "Stepped to timebag 39, now with 0 base quotes from timebag 38\n",
        "Stepped to timebag 40, now with 0 base quotes from timebag 39\n",
        "Stepped to timebag 41, now with 0 base quotes from timebag 40\n",
        "Stepped to timebag 42, now with 0 base quotes from timebag 41\n",
        "Stepped to timebag 43, now with 0 base quotes from timebag 42\n",
        "Stepped to timebag 44, now with 0 base quotes from timebag 43\n",
        "Stepped to timebag 45, now with 0 base quotes from timebag 44\n",
        "Stepped to timebag 46, now with 0 base quotes from timebag 45\n",
        "  60 % (6138 / 10225 substitutions)\n",
        "Stepped to timebag 47, now with 0 base quotes from timebag 46\n",
        "Stepped to timebag 48, now with 0 base quotes from timebag 47\n",
        "Stepped to timebag 49, now with 0 base quotes from timebag 48\n",
        "Stepped to timebag 50, now with 0 base quotes from timebag 49\n",
        "Stepped to timebag 51, now with 0 base quotes from timebag 50\n",
        "Stepped to timebag 52, now with 0 base quotes from timebag 51\n",
        "Stepped to timebag 53, now with 0 base quotes from timebag 52\n",
        "Stepped to timebag 54, now with 0 base quotes from timebag 53\n",
        "Stepped to timebag 55, now with 0 base quotes from timebag 54\n",
        "Stepped to timebag 56, now with 0 base quotes from timebag 55\n",
        "  70 % (7161 / 10225 substitutions)\n",
        "Stepped to timebag 57, now with 0 base quotes from timebag 56\n",
        "Stepped to timebag 58, now with 0 base quotes from timebag 57\n",
        "Stepped to timebag 59, now with 0 base quotes from timebag 58\n",
        "Stepped to timebag 60, now with 0 base quotes from timebag 59\n",
        "Stepped to timebag 61, now with 0 base quotes from timebag 60\n",
        "Stepped to timebag 62, now with 0 base quotes from timebag 61\n",
        "Stepped to timebag 63, now with 0 base quotes from timebag 62\n",
        "Stepped to timebag 64, now with 0 base quotes from timebag 63"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Stepped to timebag 65, now with 0 base quotes from timebag 64\n",
        "Stepped to timebag 66, now with 0 base quotes from timebag 65\n",
        "Stepped to timebag 67, now with 0 base quotes from timebag 66\n",
        "  80 % (8184 / 10225 substitutions)\n",
        "Stepped to timebag 68, now with 0 base quotes from timebag 67\n",
        "Stepped to timebag 69, now with 0 base quotes from timebag 68\n",
        "Stepped to timebag 70, now with 0 base quotes from timebag 69\n",
        "Stepped to timebag 71, now with 0 base quotes from timebag 70\n",
        "Stepped to timebag 72, now with 0 base quotes from timebag 71\n",
        "Stepped to timebag 73, now with 0 base quotes from timebag 72\n",
        "Stepped to timebag 74, now with 0 base quotes from timebag 73\n",
        "Stepped to timebag 75, now with 0 base quotes from timebag 74\n",
        "Stepped to timebag 76, now with 0 base quotes from timebag 75\n",
        "Stepped to timebag 77, now with 0 base quotes from timebag 76\n",
        "Stepped to timebag 78, now with 0 base quotes from timebag 77\n",
        "Stepped to timebag 79, now with 0 base quotes from timebag 78\n",
        "  90 % (9207 / 10225 substitutions)\n",
        "Stepped to timebag 80, now with 0 base quotes from timebag 79\n",
        "Stepped to timebag 81, now with 0 base quotes from timebag 80\n",
        "Stepped to timebag 82, now with 0 base quotes from timebag 81\n",
        "Stepped to timebag 83, now with 0 base quotes from timebag 82\n",
        "Stepped to timebag 84, now with 0 base quotes from timebag 83\n",
        "Stepped to timebag 85, now with 0 base quotes from timebag 84\n",
        "Stepped to timebag 86, now with 0 base quotes from timebag 85\n",
        "Stepped to timebag 87, now with 0 base quotes from timebag 86\n",
        "Stepped to timebag 88, now with 0 base quotes from timebag 87\n",
        "Stepped to timebag 89, now with 0 base quotes from timebag 88\n",
        "Stepped to timebag 90, now with 0 base quotes from timebag 89\n",
        "Stepped to timebag 91, now with 0 base quotes from timebag 90\n",
        "Extracted from 441 clusters.\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## A quick check\n",
      "\n",
      "Just to have a look, here we count the average number of *new* quotes that appear in the first ten timebags of each cluster (*new* according to the definition above)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.arange(1, n_tbgs_new_quotes)\n",
      "y = np_new_quotes_counts.mean(0)\n",
      "ci = 1.96 * np_new_quotes_counts.std(0) / np.sqrt(len(np_new_quotes_counts[:, 0]) - 1)\n",
      "mpl.pyplot.plot(x, np_new_quotes_counts.mean(0))\n",
      "mpl.pyplot.fill_between(x, y - ci, y + ci, alpha=0.2, color='grey')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndexError",
       "evalue": "too many indices",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-10-721a2fd6f79a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_tbgs_new_quotes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_new_quotes_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mci\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.96\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp_new_quotes_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_new_quotes_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp_new_quotes_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_between\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mci\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mci\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'grey'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mIndexError\u001b[0m: too many indices"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/sl/.virtualenvs/brainscopypaste/local/lib/python2.7/site-packages/numpy/core/_methods.py:55: RuntimeWarning: Mean of empty slice.\n",
        "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n",
        "/home/sl/.virtualenvs/brainscopypaste/local/lib/python2.7/site-packages/numpy/core/_methods.py:77: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
        "  warnings.warn(\"Degrees of freedom <= 0 for slice\", RuntimeWarning)\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Plot the results\n",
      "\n",
      "We now plot the distribution of features for novel quotes, for each feature, for a number of timebags. And see if it evolves."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# How many bins we want\n",
      "nbins = 10\n",
      "\n",
      "# How many timebags to plot\n",
      "n_timebags_shown = 10\n",
      "\n",
      "# Step between two plotted timebags\n",
      "n_timebags_step = 2\n",
      "\n",
      "# For each feature\n",
      "for fsrc, fsrc_empirical_values in empirical_features.iteritems():\n",
      "    for ftype, tbgs_empirical_values in fsrc_empirical_values.iteritems():\n",
      "        \n",
      "        # Check if we're using a log-scale\n",
      "        if st.mt_analysis_features[fsrc][ftype]['log']:\n",
      "            log = True\n",
      "            logtext = ' (log)'\n",
      "        else:\n",
      "            log = False\n",
      "            logtext = ''\n",
      "        \n",
      "        # Create axes\n",
      "        fig1, ax1 = mpl.pyplot.subplots(1, 1, figsize=(20, 4))\n",
      "        fig1.text(0.5, 1, '{} {}'.format(fsrc, ftype.replace('_', ' ')) + logtext)\n",
      "        fig2, axs2 = mpl.pyplot.subplots(1, n_timebags_shown, figsize=(20, 3), sharex=True)\n",
      "        \n",
      "        # Convert values\n",
      "        vals = []\n",
      "        for lvals in tbgs_empirical_values[:n_timebags_shown * n_timebags_step:n_timebags_step]:\n",
      "            \n",
      "            if log:\n",
      "                vals.append(np.log(lvals))\n",
      "            else:\n",
      "                vals.append(lvals)\n",
      "            \n",
      "        names = ['tbg {}'.format(i) for i in range(0, n_timebags_shown * n_timebags_step, n_timebags_step)]\n",
      "        \n",
      "        # We want the same bandwidth in all violins for a given feature\n",
      "        # n**(-1./(d+4)) where n is the number of data points,\n",
      "        # and d the number of dimensions (Scott's rule)\n",
      "        bw = len(vals[0]) ** (- 1. / (1 + 4))\n",
      "        sb.violinplot(vals, names=names, bw=bw, ax=ax1)#, log=log)\n",
      "        \n",
      "        # Add histograms\n",
      "        for i in range(n_timebags_shown):\n",
      "            if i == 0:\n",
      "                bins = axs2[i].hist(vals[i], bins=nbins,# log=log,\n",
      "                                    normed=True, color=\"#6495ED\")[1]\n",
      "            else:\n",
      "                axs2[i].hist(vals[i], bins=bins,# log=log,\n",
      "                             normed=True, color=\"#6495ED\")\n",
      "            axs2[i].set_xlabel('tbg {} ({} pts)'.format(i * n_timebags_step, len(vals[i])))\n",
      "            axs2[i].set_xlim([bins.min(), bins.max()])\n",
      "            axs2[i].set_xticklabels(axs2[i].get_xticks(), rotation=60)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}